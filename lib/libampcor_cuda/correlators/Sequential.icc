// -*- C++ -*-
// -*- coding: utf-8 -*-
//
// michael a.g. aïvázis <michael.aivazis@para-sim.com>
// parasim
// (c) 1998-2019 all rights reserved
//

// code guard
#if !defined(ampcor_cuda_correlators_Sequential_icc)
#error This header is an implementation detail of ampcor::cuda::correlators::Sequential
#endif

// interface
template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
addReferenceTile(size_type pid, const constview_type & ref)
{
    // figure out the starting address of this tile in the arena
    cell_type * support = _arena + pid*(_refCells + _tgtCells);
    // adapt it into a grid
    tile_type tile(_refLayout, support);
    // move the data
    std::copy(ref.begin(), ref.end(), tile.view().begin());
    // all done
    return;
}


template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
addTargetTile(size_type pid, const constview_type & tgt)
{
    // figure out the starting address of this tile in the arena
    cell_type * support = _arena + pid*(_refCells + _tgtCells) + _refCells;
    // adapt it into a grid
    tile_type tile(_tgtLayout, support);
    // move the data
    std::copy(tgt.begin(), tgt.end(), tile.view().begin());
    // all done
    return;
}


template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
adjust()
{
    // move the tiles to the device
    auto coarseArena = _push();
    // compute their amplitudes
    auto amplitudes = _detect(coarseArena);
    // adjust the reference tiles to zero mean and compute the variances
    auto refStatistics = _refStats(amplitudes);
    // compute the sum area tables for all possible search window placements in the target tile
    auto sat = _sat(amplitudes);
    // use the SATs to compute the mean amplitude of all possible window placements
    auto tgtStatistics = _tgtStats(sat);
    // compute the correlation hyper-surface
    auto gamma = _correlate(amplitudes, refStatistics, tgtStatistics);
    // find its maxima
    auto maxcor = _maxcor(gamma);
    // ensure that the expanded target tiles where the correlation is maximum fit within the
    // search window
    _nudge(maxcor);

    // allocate room for the refinement area
    auto refinedArena = _refinedArena();
    // refine the reference tiles
    _refRefine(coarseArena, refinedArena);
    // collect the expanded maxcor tiles
    _tgtMigrate(coarseArena, maxcor, refinedArena);
    // refine the target tiles
    _tgtRefine(refinedArena);

    // clean up
    cudaFree(maxcor);
    cudaFree(gamma);
    cudaFree(tgtStatistics);
    cudaFree(sat);
    cudaFree(refStatistics);
    cudaFree(amplitudes);
    cudaFree(coarseArena);

    // all done
    return;
}


// accessors
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
arena() const -> const cell_type *
{
    return _arena;
}


// meta-methods
template <typename raster_t>
ampcor::cuda::correlators::Sequential<raster_t>::
~Sequential() {
    // release the host memory
    delete [] _arena;
}


template <typename raster_t>
ampcor::cuda::correlators::Sequential<raster_t>::
Sequential(size_type pairs,
           const layout_type & refLayout, const layout_type & tgtLayout,
           size_type refineFactor, size_type refineMargin) :
    _pairs{ pairs },
    _refineFactor{ refineFactor },
    _refineMargin{ refineMargin },
    _refLayout{ refLayout },
    _tgtLayout{ tgtLayout },
    _corLayout{ tgtLayout.shape() - refLayout.shape() + index_type::fill(1) },
    _refRefinedLayout{ refineFactor * _refLayout.shape() },
    _tgtRefinedLayout{ refineFactor * (_refLayout.shape() + index_type::fill(2*refineMargin)) },
    _refCells{ _refLayout.size() },
    _tgtCells{ _tgtLayout.size() },
    _corCells{ _corLayout.size() },
    _refRefinedCells{ _refRefinedLayout.size() },
    _tgtRefinedCells{ _tgtRefinedLayout.size() },
    _refFootprint{ _refCells * sizeof(cell_type) },
    _tgtFootprint{ _tgtCells * sizeof(cell_type) },
    _corFootprint{ _corCells * sizeof(value_type) }, // the correlation matrix is real...
    _refRefinedFootprint{ _refRefinedCells * sizeof(cell_type) },
    _tgtRefinedFootprint{ _tgtRefinedCells * sizeof(cell_type) },
    _arena{ new cell_type[ _pairs * (_refCells+_tgtCells) ] }
{
    // compute the footprints
    auto footprint = _pairs*(_refFootprint + _tgtFootprint);
    auto refinedFootprint = _pairs*(_refRefinedFootprint + _tgtRefinedFootprint);
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");
    // show me
    channel
        << pyre::journal::at(__HERE__)
        << "new Sequential worker:"
        << pyre::journal::newline
        << "    pairs: " << _pairs
        << pyre::journal::newline
        << "    ref shape: " << _refLayout << ", " << _refCells << " cells"
        << pyre::journal::newline
        << "    tgt shape: " << _tgtLayout << ", " << _tgtCells << " cells"
        << pyre::journal::newline
        << "    footprint: " << (_refCells+_tgtCells) << " cells in "
        << (footprint/1024/1024) << " Mb"
        << pyre::journal::newline
        << "    refine factor: " << refineFactor
        << pyre::journal::newline
        << "    refine margin: " << refineMargin
        << pyre::journal::newline
        << "    refined ref shape: " << _refRefinedLayout << ", " << _refRefinedCells << " cells"
        << pyre::journal::newline
        << "    refined tgt shape: " << _tgtRefinedLayout << ", " << _tgtRefinedCells << " cells"
        << pyre::journal::newline
        << "    footprint: " << (_refRefinedCells+_tgtRefinedCells) << " cells in "
        << (refinedFootprint/1024/1024) << " Mb"
        << pyre::journal::newline
        << "    arena: " << _arena
        << pyre::journal::endl;
}


// debugging support
template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
dump() const
{
    // dump the arena as a sequence of reference and target tiles
    pyre::journal::debug_t channel("ampcor.cuda");

    // sign in
    channel << pyre::journal::at(__HERE__);
    // go through all the pairs
    for (auto pid = 0; pid < _pairs; ++pid) {
        // inject the pid
        channel << "pid: " << pid << pyre::journal::newline;
        // compute the address of the reference tile in the arena
        cell_type * refLoc = _arena + pid*(_refCells + _tgtCells);
        // adapt it into a grid
        tile_type ref(_refLayout, refLoc);
        // inject it
        channel << "reference: " << pyre::journal::newline;
        for (auto idx = 0; idx < _refLayout.shape()[0]; ++idx) {
            for (auto jdx = 0; jdx < _refLayout.shape()[1]; ++jdx) {
                channel << ref[{idx, jdx}] << " ";
            }
            channel << pyre::journal::newline;
        }

        // compute the address of the target tile in the arena
        cell_type * tgtLoc = refLoc + _refCells;
        // adapt it into a grid
        tile_type tgt(_tgtLayout, tgtLoc);

        // inject it
        channel << "target: " << pyre::journal::newline;
        for (auto idx = 0; idx < _tgtLayout.shape()[0]; ++idx) {
            for (auto jdx = 0; jdx < _tgtLayout.shape()[1]; ++jdx) {
                channel << tgt[{idx, jdx}] << " ";
            }
            channel << pyre::journal::newline;
        }

    }
    // sing off
    channel << pyre::journal::endl;

    // all done
    return;
}


// implementation details: methods

// push the tiles to the device
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_push() const -> cell_type *
{
    // grab a spot
    cell_type * cArena = nullptr;
    // compute the required size
    auto footprint = _pairs * (_refFootprint + _tgtFootprint);
    // allocate room for it
    auto status = cudaMallocManaged(&cArena, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating " << 1.0*footprint/1024/1024
            << "Mb of device memory for the input hyper-grid: "
            << cudaGetErrorName(status) << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }

    // move the data
    status = cudaMemcpy(cArena, _arena, footprint, cudaMemcpyHostToDevice);
    // if something went wrong
    if (status != cudaSuccess) {
        // build the error description
        std::string description = cudaGetErrorName(status);
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while transferring tiles to the device: "
            << description << " (" << status << ")"
            << pyre::journal::endl;
        // release the memory
        cudaFree(cArena);
        // and bail
        throw std::logic_error(description);
    }

    // all done
    return cArena;
}

// compute the amplitude of the signal
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_detect(const cell_type * cArena) const -> value_type *
{
    // find a spot on the device
    value_type * rArena = nullptr;
    // compute the number of cells whose amplitude we have to compute
    auto cells = _pairs * (_refCells + _tgtCells);
    // compute the required size
    auto footprint = cells * sizeof(value_type);
    // allocate room for it
    auto status = cudaMallocManaged(&rArena, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating " << 1.0*footprint/1024/1024
            << "Mb of device memory for the tile amplitudes: "
            << cudaGetErrorName(status) << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }

    // engage...
    kernels::detect(cArena, cells, rArena);

    // all done
    return rArena;
};

// subtract the tile mean from each reference pixel
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_refStats(value_type * rArena) const -> value_type *
{
    // grab a spot
    value_type * stats = nullptr;
    // compute the amount of memory needed to store the variances of the reference tiles: one
    // number per reference tile
    auto footprint = _pairs * sizeof(value_type);
    // allocate room
    auto status = cudaMallocManaged(&stats, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating " << 1.0*footprint/1024/1024
            << "Mb of device memory for the variances of the reference tiles: "
            << cudaGetErrorName(status) << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }

    // get the layout of the reference tiles; assume they are square
    auto refExt = _refLayout.shape()[0];
    // engage
    kernels::refStats(rArena, _pairs, refExt, _refCells + _tgtCells, stats);

    // all done
    return stats;
}

// build the sum area tables for the target tiles
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_sat(const value_type * rArena) const -> value_type *
{
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");

    // grab a spot for the sat tables
    value_type * sat = nullptr;
    // compute the amount of memory needed to store them
    auto footprint = _pairs * _tgtFootprint;
    // allocate memory
    auto status = cudaMallocManaged(&sat, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating memory for the sum area tables: "
            << cudaGetErrorName(status) << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }
    // otherwise, show me
    channel
        << pyre::journal::at(__HERE__)
        << "allocated an arena of " << footprint << " bytes for the sum area tables at "
        << sat
        << pyre::journal::endl;

    // compute the layout of the target tiles; assume they are square
    auto tgtDim = _tgtLayout.shape()[0];
    // engage
    kernels::sat(rArena, _pairs, _refCells, _tgtCells, tgtDim, sat);

    // all done
    return sat;
}


// compute the average values for all possible placements of the reference shape within the
// target tile
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_tgtStats(const value_type * dSAT) const -> value_type *
{
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");

    // pick a spot for the table of amplitude averages
    value_type * stats = nullptr;
    // compute the amount of memory needed to store the target tile statistics: we store the
    // mean per placement per tile
    auto footprint = _pairs * _corFootprint;
    // allocate memory on the device
    auto status = cudaMallocManaged(&stats, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // get the error description
        std::string description = cudaGetErrorName(status);
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating device memory for the table of target amplitude averages: "
            << description << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }
    // show me
    channel
        << pyre::journal::at(__HERE__)
        << "allocated an arena of " << footprint << " bytes for the target amplitude averages at "
        << stats
        << pyre::journal::endl;

    // get the dimension of the reference tiles
    auto refDim = _refLayout.shape()[0];
    // get the dimension of the target tiles
    auto tgtDim = _tgtLayout.shape()[0];
    // get the dimensions of the correlation matrix
    auto corDim = _corLayout.shape()[0];
    // engage
    kernels::tgtStats(dSAT, _pairs, refDim, tgtDim, corDim, stats);

    // all done
    return stats;
}


// compute the correlation surface
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_correlate(const value_type * rArena,
           const value_type * refStats,
           const value_type * tgtStats) const -> value_type *
{
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");

    // pick a spot for the correlation matrix
    value_type * dCorrelation = nullptr;
    // compute the total number of cells in the amplitude hyper-grid
    auto size = _pairs * _corCells;
    // and the amount of memory needed to store them
    auto footprint = _pairs * _corFootprint;
    // allocate memory on the device
    auto status = cudaMallocManaged(&dCorrelation, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // get the error description
        std::string description = cudaGetErrorName(status);
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating device memory for the correlation matrix: "
            << description << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }
    // show me
    channel
        << pyre::journal::at(__HERE__)
        << "allocated " << footprint << " bytes for the correlation matrix at "
        << dCorrelation
        << pyre::journal::endl;

    // get the dimension of the reference tiles
    auto refDim = _refLayout.shape()[0];
    // get the dimension of the target tiles
    auto tgtDim = _tgtLayout.shape()[0];
    // get the dimension of the correlation matrix
    auto corDim = _corLayout.shape()[0];

    // engage
    kernels::correlate(rArena, refStats, tgtStats,
                       _pairs,
                       _refCells, _tgtCells, _corCells, refDim, tgtDim, corDim,
                       dCorrelation);

    // all done
    return dCorrelation;
}


// find the locations of the correlation maxima
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_maxcor(const value_type * gamma) const -> int *
{
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");

    // find a spot
    int * loc = nullptr;
    // compute the amount of memory we need: a (row, coal) locator for each tile pair
    auto footprint = 2 * _pairs * sizeof(int);
    // allocate memory on the device
    auto status = cudaMallocManaged(&loc, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // get the error description
        std::string description = cudaGetErrorName(status);
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating device memory for the location of the correlation maxima: "
            << description << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }
    // show me
    channel
        << pyre::journal::at(__HERE__)
        << "allocated " << footprint << " bytes for the locations of the correlation maxima at "
        << loc
        << pyre::journal::endl;

    // get the dimension of the correlation matrix
    auto corDim = _corLayout.shape()[0];
    // engage
    kernels::maxcor(gamma, _pairs, _corCells, corDim, loc);

    // all done
    return loc;
}


// adjust the locations of the correlation maxima so that the new target tiles fit within the
// search window
template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
_nudge(int * locations) const
{
    // make sure that all locations are adjusted so that they allow enough room for the
    // {refineMargin}

    // get the dimension of the reference tiles
    auto refDim = _refLayout.shape()[0];
    // get the dimension of the target tiles
    auto tgtDim = _tgtLayout.shape()[0];
    // move the ULHC of the tiles so they fit
    kernels::nudge(_pairs, refDim, tgtDim, _refineMargin, locations);

    // all done
    return;
}


// allocate room for the refined tiles
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_refinedArena() const -> cell_type *
{
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");

    // grab a spot
    cell_type * arena = nullptr;
    // compute the required size
    auto footprint = _pairs * (_refRefinedFootprint + _tgtRefinedFootprint);
    // allocate room for it
    auto status = cudaMallocManaged(&arena, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating " << 1.0*footprint/1024/1024
            << "Mb of device memory for the refined tile hyper-grid: "
            << cudaGetErrorName(status) << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }

    // show me
    channel
        << pyre::journal::at(__HERE__)
        << "allocated a refined arena: " << footprint << " bytes at " << arena
        << pyre::journal::endl;

    // initialize the memory
    status = cudaMemset(arena, 0, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // get the error description
        std::string description = cudaGetErrorName(status);
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while initializing " << footprint
            << " bytes of device memory for the refined tile hyper-grid: "
            << description << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::runtime_error(description);
    }

    // all done
    return arena;
}


// refine the reference tiles
template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
_refRefine(cell_type * coarseArena, cell_type * refinedArena) const
{
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");

    // get the shape the reference tile
    auto rshape = _refLayout.shape();
    // and the shape of the refined tile
    auto tshape = _refRefinedLayout.shape();

    // step 1: forward FFT from {coarseArena} to {refinedArena}
    // the plan characteristics
    int dim = 2;
    int fwdRanks[] = { static_cast<int>(rshape[0]), static_cast<int>(rshape[1]) };
    //  the data layout of the coarse tiles
    // the tile shape stays the same
    int fwdIEmbed[] = { static_cast<int>(rshape[0]), static_cast<int>(rshape[1]) };
    // the data is densely packed
    int fwdIStride = 1;
    // the distance between reference tiles
    int fwdIDist = _refCells + _tgtCells;
    // the data layout of the refined tiles
    // the tile shape stays the same
    int fwdOEmbed[] = { static_cast<int>(tshape[0]), static_cast<int>(tshape[1]) };
    // the data is densely packed
    int fwdOStride = 1;
    // the distance between reference tiles in the refined arena
    int fwdODist = _refRefinedCells + _tgtRefinedCells;

    // grab a spot for a plan
    cufftHandle fwdPlan;
    // instantiate
    auto status = cufftPlanMany(&fwdPlan, dim, fwdRanks,
                                fwdIEmbed, fwdIStride, fwdIDist,
                                fwdOEmbed, fwdOStride, fwdODist,
                                CUFFT_C2C,
                                _pairs
                                );
    // if something went wrong
    if (status != CUFFT_SUCCESS) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while refining the reference tiles: forward FFT plan: error " << status
            << pyre::journal::endl;
        // and bail
        throw std::runtime_error("while refining the reference tiles: forward FFT plan error");
    }

    // execute the forward plan
    status = cufftExecC2C(fwdPlan,
                          reinterpret_cast<cufftComplex *>(coarseArena),
                          reinterpret_cast<cufftComplex *>(refinedArena),
                          CUFFT_FORWARD);
    // if something went wrong
    if (status != CUFFT_SUCCESS) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while refining the reference tiles: executing the forward FFT plan: error "
            << status
            << pyre::journal::endl;
        // and bail
        throw std::runtime_error("while executing the forward FFT plan for the reference tiles");
    }
    // wait for the device to finish
    auto code = cudaDeviceSynchronize();
    // if something went wrong
    if (code != cudaSuccess) {
        // form the error description
        std::string description = cudaGetErrorName(code);
        // make a channel
        pyre::journal::error_t channel("ampcor.cuda");
        // complain
        channel
            << pyre::journal::at(__HERE__)
            << "while refining reference tiles: STEP 1: " << description << " (" << code << ")"
            << pyre::journal::endl;
        // bail
        throw std::runtime_error(description);
    }
    // clean up
    cufftDestroy(fwdPlan);

    // step 2: reverse FFT of the refined tiles back into the refined arena
    // the plan characteristics
    int revRanks[] = { static_cast<int>(tshape[0]), static_cast<int>(tshape[1]) };
    // the data layout of the transformed reference tiles
    int revIEmbed[] = { static_cast<int>(tshape[0]), static_cast<int>(tshape[1]) };
    // the data is densely packed
    int revIStride = 1;
    // the distance between reference tiles
    int revIDist = _refRefinedCells + _tgtRefinedCells;
    // the reverse FFT tiles have identical layout
    int revOEmbed[] = { static_cast<int>(tshape[0]), static_cast<int>(tshape[1]) };
    // the data is densely packed
    int revOStride = 1;
    // the distance between reference tiles
    int revODist = _refRefinedCells + _tgtRefinedCells;

    // grab a spot
    cufftHandle revPlan;
    // instantiate the reverse plan
    status = cufftPlanMany(&revPlan, dim, revRanks,
                           revIEmbed, revIStride, revIDist,
                           revOEmbed, revOStride, revODist,
                           CUFFT_C2C,
                           _pairs
                           );
    // if something went wrong
    if (status != CUFFT_SUCCESS) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while refining the reference tiles: reverse FFT plan: error " << status
            << pyre::journal::endl;
        // and bail
        throw std::runtime_error("while refining the reference tiles: reverse FFT plan error");
    }

    // execute the reverse plan
    status = cufftExecC2C(revPlan,
                          reinterpret_cast<cufftComplex *>(refinedArena),
                          reinterpret_cast<cufftComplex *>(refinedArena),
                          CUFFT_INVERSE);
    // if something went wrong
    if (status != CUFFT_SUCCESS) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while refining the reference tiles: executing the reverse FFT plan: error "
            << status
            << pyre::journal::endl;
        // and bail
        throw std::runtime_error("while executing the reverse FFT plan for the reference tiles");
    }
    // wait for the device to finish
    code = cudaDeviceSynchronize();
    // if something went wrong
    if (code != cudaSuccess) {
        // form the error description
        std::string description = cudaGetErrorName(code);
        // make a channel
        pyre::journal::error_t channel("ampcor.cuda");
        // complain
        channel
            << pyre::journal::at(__HERE__)
            << "while refining reference tiles: STEP 2: " << description << " (" << code << ")"
            << pyre::journal::endl;
        // bail
        throw std::runtime_error(description);
    }

    // clean up
    cufftDestroy(revPlan);

    // all done
    return;
}


// migrate the expanded unrefined target tiles into the {refinedArena}
template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
_tgtMigrate(cell_type * coarseArena, int * locations, cell_type * refinedArena) const
{
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");

    // the reference tile shape
    auto refShape = _refLayout.shape();
    // the target tile shape
    auto tgtShape = _tgtLayout.shape();
    // the refined reference tile shape
    auto refRefinedShape = _refRefinedLayout.shape();
    // the refined target tile shape
    auto tgtRefinedShape = _tgtRefinedLayout.shape();

    // unpack the dimensions
    auto refDim = refShape[0];
    auto tgtDim = tgtShape[0];
    auto refRefinedDim = refRefinedShape[0];
    auto tgtRefinedDim = tgtRefinedShape[0];

    // compute the dimension of the expanded maxcor tile
    auto expDim = refDim + 2 * _refineMargin;

    // engage...
    kernels::migrate(coarseArena, _pairs,
                     refDim, tgtDim, expDim,
                     refRefinedDim, tgtRefinedDim,
                     locations,
                     refinedArena);

    // all done
    return;
}


// refine the target tiles around the locations of the correlation maxima
template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
_tgtRefine(cell_type * refinedArena) const
{
    // N.B.: assumes {locations} are already nudged and on the CPU...
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");

    // the shape of the target tile
    auto tgtShape = _tgtLayout.shape();
    // the shape the refined target tile
    auto tgtRefShape = _tgtRefinedLayout.shape();
    // the shape of the expanded target tile
    auto expShape = _refLayout.shape() + index_type::fill(2*_refineMargin);

    // N.B.: the expanded maxcor target tiles are expected to have already been moved to the
    // refined arena after the maxcor locations were determined

    // step 1: forward FFT in place in {refinedArena}
    // the plan characteristics
    int dim = 2;
    // use the shape of the expanded target tile
    int fwdRanks[] = { static_cast<int>(expShape[0]), static_cast<int>(expShape[1]) };
    // this tile is already occupying its destination location in {refinedArena}
    int fwdIEmbed[] = { static_cast<int>(tgtRefShape[0]), static_cast<int>(tgtRefShape[1]) };
    // the data is dense
    int fwdIStride = 1;
    // the distance between tiles
    int fwdIDist = _refRefinedCells + _tgtRefinedCells;
    // the destination of the forward FFT has indentical layout
    int fwdOEmbed[] = { static_cast<int>(tgtRefShape[0]), static_cast<int>(tgtRefShape[1]) };
    // the data is dense
    int fwdOStride = 1;
    // the distance between tiles
    int fwdODist = _refRefinedCells + _tgtRefinedCells;

    // grab a spot for a plan
    cufftHandle fwdPlan;
    // instantiate
    auto status = cufftPlanMany(&fwdPlan, dim, fwdRanks,
                                fwdIEmbed, fwdIStride, fwdIDist,
                                fwdOEmbed, fwdOStride, fwdODist,
                                CUFFT_C2C,
                                _pairs
                                );
    // if something went wrong
    if (status != CUFFT_SUCCESS) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while refining the target tiles: forward FFT plan: error " << status
            << pyre::journal::endl;
        // and bail
        throw std::runtime_error("while refining the target tiles: forward FFT plan error");
    }

    // the address of the first expanded maxcor tile
    auto firstTile = reinterpret_cast<cufftComplex *>(refinedArena + _refRefinedCells);
    // execute the forward plan
    status = cufftExecC2C(fwdPlan, firstTile, firstTile, CUFFT_FORWARD);
    // if something went wrong
    if (status != CUFFT_SUCCESS) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while refining the target tiles: executing the forward FFT plan: error "
            << status
            << pyre::journal::endl;
        // and bail
        throw std::runtime_error("while executing the forward FFT plan for the reference tiles");
    }
    // wait for the device to finish
    auto code = cudaDeviceSynchronize();
    // if something went wrong
    if (code != cudaSuccess) {
        // form the error description
        std::string description = cudaGetErrorName(code);
        // make a channel
        pyre::journal::error_t channel("ampcor.cuda");
        // complain
        channel
            << pyre::journal::at(__HERE__)
            << "while refining target tiles: STEP 1: " << description << " (" << code << ")"
            << pyre::journal::endl;
        // bail
        throw std::runtime_error(description);
    }
    // clean up the forward plan
    cufftDestroy(fwdPlan);

    // step 2: reverse FFT of the refined tiles back into the refined arena
    // the plan characteristics
    int revRanks[] = { static_cast<int>(tgtRefShape[0]), static_cast<int>(tgtRefShape[1]) };

    // the data layout of the transformed reference tiles
    int revIEmbed[] = { static_cast<int>(tgtRefShape[0]), static_cast<int>(tgtRefShape[1]) };
    // the data is densely packed
    int revIStride = 1;
    // the distance between reference tiles
    int revIDist = _refRefinedCells + _tgtRefinedCells;
    // the reverse FFT tiles have identical layout
    int revOEmbed[] = { static_cast<int>(tgtRefShape[0]), static_cast<int>(tgtRefShape[1]) };
    // the data is densely packed
    int revOStride = 1;
    // the distance between reference tiles
    int revODist = _refRefinedCells + _tgtRefinedCells;

    // grab a spot
    cufftHandle revPlan;
    // instantiate the reverse plan
    status = cufftPlanMany(&revPlan, dim, revRanks,
                           revIEmbed, revIStride, revIDist,
                           revOEmbed, revOStride, revODist,
                           CUFFT_C2C,
                           _pairs
                           );
    // if something went wrong
    if (status != CUFFT_SUCCESS) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while refining the target tiles: reverse FFT plan: error " << status
            << pyre::journal::endl;
        // and bail
        throw std::runtime_error("while refining the target tiles: reverse FFT plan error");
    }

    // execute the reverse plan
    status = cufftExecC2C(revPlan, firstTile, firstTile, CUFFT_FORWARD);
    // if something went wrong
    if (status != CUFFT_SUCCESS) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while refining the target tiles: executing the reverse FFT plan: error "
            << status
            << pyre::journal::endl;
        // and bail
        throw std::runtime_error("while executing the reverse FFT plan for the target tiles");
    }
    // wait for the device to finish
    code = cudaDeviceSynchronize();
    // if something went wrong
    if (code != cudaSuccess) {
        // form the error description
        std::string description = cudaGetErrorName(code);
        // make a channel
        pyre::journal::error_t channel("ampcor.cuda");
        // complain
        channel
            << pyre::journal::at(__HERE__)
            << "while refining target tiles: STEP 2: " << description << " (" << code << ")"
            << pyre::journal::endl;
        // bail
        throw std::runtime_error(description);
    }

    // clean up
    cufftDestroy(revPlan);

    // all done
    return;
}


// end of file
