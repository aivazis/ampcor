// -*- C++ -*-
// -*- coding: utf-8 -*-
//
// michael a.g. aïvázis <michael.aivazis@para-sim.com>
// parasim
// (c) 1998-2019 all rights reserved
//

// code guard
#if !defined(ampcor_cuda_correlators_Sequential_icc)
#error This header is an implementation detail of ampcor::cuda::correlators::Sequential
#endif

// interface
template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
addReferenceTile(size_type pid, const constview_type & ref)
{
    // figure out the starting address of this tile in the arena
    cell_type * support = _arena + pid*(_refCells + _tgtCells);
    // adapt it into a grid
    tile_type tile(_refLayout, support);
    // move the data
    std::copy(ref.begin(), ref.end(), tile.view().begin());
    // all done
    return;
}

template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
addTargetTile(size_type pid, const constview_type & tgt)
{
    // figure out the starting address of this tile in the arena
    cell_type * support = _arena + pid*(_refCells + _tgtCells) + _refCells;
    // adapt it into a grid
    tile_type tile(_tgtLayout, support);
    // move the data
    std::copy(tgt.begin(), tgt.end(), tile.view().begin());
    // all done
    return;
}

template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
adjust()
{
    // all done
    return;
}

template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
refine()
{
    // all done
    return;
}


// accessors
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
arena() const -> const cell_type *
{
    return _arena;
}


// meta-methods
template <typename raster_t>
ampcor::cuda::correlators::Sequential<raster_t>::
~Sequential() {
    // release the host memory
    delete [] _arena;
}


template <typename raster_t>
ampcor::cuda::correlators::Sequential<raster_t>::
Sequential(size_type pairs, const layout_type & refLayout, const layout_type & tgtLayout) :
    _pairs{pairs},
    _refLayout{ refLayout },
    _tgtLayout{ tgtLayout },
    _corLayout{ tgtLayout.shape() - refLayout.shape() + index_type::fill(1) },
    _refCells{ _refLayout.size() },
    _tgtCells{ _tgtLayout.size() },
    _corCells{ _corLayout.size() },
    _refFootprint{ _refCells * sizeof(cell_type) },
    _tgtFootprint{ _tgtCells * sizeof(cell_type) },
    _corFootprint{ _corCells * sizeof(value_type) }, // the correlation matrix is real...
    _arena{ new cell_type[ _pairs * (_refCells+_tgtCells) ] }
{
    // compute the footprint
    auto footprint = _pairs*(_refFootprint + _tgtFootprint);
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");
    // show me
    channel
        << pyre::journal::at(__HERE__)
        << "new Sequential worker:"
        << pyre::journal::newline
        << "    pairs: " << _pairs
        << pyre::journal::newline
        << "    ref shape: " << _refLayout << ", " << _refCells << " cells"
        << pyre::journal::newline
        << "    tgt shape: " << _tgtLayout << ", " << _tgtCells << " cells"
        << pyre::journal::newline
        << "    footprint: " << (_refCells+_tgtCells) << " cells in "
        << (footprint/1024/1024) << " Mb"
        << pyre::journal::newline
        << "    arena: " << _arena
        << pyre::journal::endl;
}


// debugging support
template <typename raster_t>
void
ampcor::cuda::correlators::Sequential<raster_t>::
dump() const
{
    // dump the arena as a sequence of reference and target tiles
    pyre::journal::debug_t channel("ampcor.cuda");

    // sign in
    channel << pyre::journal::at(__HERE__);
    // go through all the pairs
    for (auto pid = 0; pid < _pairs; ++pid) {
        // inject the pid
        channel << "pid: " << pid << pyre::journal::newline;
        // compute the address of the reference tile in the arena
        cell_type * refLoc = _arena + pid*(_refCells + _tgtCells);
        // adapt it into a grid
        tile_type ref(_refLayout, refLoc);
        // inject it
        channel << "reference: " << pyre::journal::newline;
        for (auto idx = 0; idx < _refLayout.shape()[0]; ++idx) {
            for (auto jdx = 0; jdx < _refLayout.shape()[1]; ++jdx) {
                channel << ref[{idx, jdx}] << " ";
            }
            channel << pyre::journal::newline;
        }

        // compute the address of the target tile in the arena
        cell_type * tgtLoc = refLoc + _refCells;
        // adapt it into a grid
        tile_type tgt(_tgtLayout, tgtLoc);

        // inject it
        channel << "target: " << pyre::journal::newline;
        for (auto idx = 0; idx < _tgtLayout.shape()[0]; ++idx) {
            for (auto jdx = 0; jdx < _tgtLayout.shape()[1]; ++jdx) {
                channel << tgt[{idx, jdx}] << " ";
            }
            channel << pyre::journal::newline;
        }

    }
    // sing off
    channel << pyre::journal::endl;

    // all done
    return;
}


// implementation details: methods

// push the tiles to the device
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_push() const -> cell_type *
{
    // grab a spot
    cell_type * cArena = nullptr;
    // compute the required size
    auto footprint = _pairs * (_refFootprint + _tgtFootprint);
    // allocate room for it
    cudaError_t status = cudaMallocManaged(&cArena, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating " << 1.0*footprint/1024/1024
            << "Mb of device memory for the input hyper-grid: "
            << cudaGetErrorName(status) << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }

    // move the data
    status = cudaMemcpy(cArena, _arena, footprint, cudaMemcpyHostToDevice);
    // if something went wrong
    if (status != cudaSuccess) {
        // build the error description
        std::string description = cudaGetErrorName(status);
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while transferring tiles to the device: "
            << description << " (" << status << ")"
            << pyre::journal::endl;
        // release the memory
        cudaFree(cArena);
        // and bail
        throw std::logic_error(description);
    }

    // all done
    return cArena;
}

// compute the amplitude of the signal
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_detect(const cell_type * cArena) const -> value_type *
{
    // find a spot on the device
    value_type * rArena = nullptr;
    // compute the number of cells whose amplitude we have to compute
    auto cells = _pairs * (_refCells + _tgtCells);
    // compute the required size
    auto footprint = cells * sizeof(value_type);
    // allocate room for it
    cudaError_t status = cudaMallocManaged(&rArena, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating " << 1.0*footprint/1024/1024
            << "Mb of device memory for the tile amplitudes: "
            << cudaGetErrorName(status) << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }

    // engage...
    kernels::detect(cArena, cells, rArena);

    // all done
    return rArena;
};

// subtract the tile mean from each reference pixel
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_refStats(value_type * rArena) const -> value_type *
{
    // grab a spot
    value_type * stats = nullptr;
    // compute the amount of memory needed to store the variances of the reference tiles: one
    // number per reference tile
    auto footprint = _pairs * sizeof(value_type);
    // allocate room
    cudaError_t status = cudaMallocManaged(&stats, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating " << 1.0*footprint/1024/1024
            << "Mb of device memory for the variances of the reference tiles: "
            << cudaGetErrorName(status) << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }

    // get the layout of the reference tiles; assume they are square
    auto refExt = _refLayout.shape()[0];
    // engage
    kernels::refStats(rArena, _pairs, refExt, _refCells + _tgtCells, stats);

    // all done
    return stats;
}

// build the sum area tables for the target tiles
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_sat(const value_type * rArena) const -> value_type *
{
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");

    // grab a spot for the sat tables
    value_type * sat = nullptr;
    // compute the amount of memory needed to store them
    auto footprint = _pairs * _tgtFootprint;
    // allocate memory
    cudaError_t status = cudaMallocManaged(&sat, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating memory for the sum area tables: "
            << cudaGetErrorName(status) << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }
    // otherwise, show me
    channel
        << pyre::journal::at(__HERE__)
        << "allocated an arena of " << footprint << " bytes for the sum area tables at "
        << sat
        << pyre::journal::endl;

    // compute the layout of the target tiles; assume they are square
    auto tgtDim = _tgtLayout.shape()[0];
    // engage
    kernels::sat(rArena, _pairs, _refCells, _tgtCells, tgtDim, sat);

    // all done
    return sat;
}


// compute the average values for all possible placements of the reference shape within the
// target tile
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_tgtStats(const value_type * dSAT) const -> value_type *
{
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");

    // pick a spot for the table of amplitude averages
    value_type * stats = nullptr;
    // compute the amount of memory needed to store the target tile statistics: we store the
    // mean per placement per tile
    auto footprint = _pairs * _corFootprint;
    // allocate memory on the device
    cudaError_t status = cudaMallocManaged(&stats, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // get the error description
        std::string description = cudaGetErrorName(status);
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating device memory for the table of target amplitude averages: "
            << description << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }
    // show me
    channel
        << pyre::journal::at(__HERE__)
        << "allocated an arena of " << footprint << " bytes for the target amplitude averages at "
        << stats
        << pyre::journal::endl;

    // get the dimension of the reference tiles
    auto refDim = _refLayout.shape()[0];
    // get the dimension of the target tiles
    auto tgtDim = _tgtLayout.shape()[0];
    // get the dimensions of the correlation matrix
    auto corDim = _corLayout.shape()[0];
    // engage
    kernels::tgtStats(dSAT, _pairs, refDim, tgtDim, corDim, stats);

    // all done
    return stats;
}


// compute the correlation surface
template <typename raster_t>
auto
ampcor::cuda::correlators::Sequential<raster_t>::
_correlate(const value_type * rArena,
           const value_type * refStats,
           const value_type * tgtStats) const -> value_type *
{
    // make a channel
    pyre::journal::debug_t channel("ampcor.cuda");

    // pick a spot for the correlation matrix
    value_type * dCorrelation = nullptr;
    // compute the total number of cells in the amplitude hyper-grid
    auto size = _pairs * _corCells;
    // and the amount of memory needed to store them
    auto footprint = _pairs * _corFootprint;
    // allocate memory on the device
    cudaError_t status = cudaMallocManaged(&dCorrelation, footprint);
    // if something went wrong
    if (status != cudaSuccess) {
        // get the error description
        std::string description = cudaGetErrorName(status);
        // make a channel
        pyre::journal::error_t error("ampcor.cuda");
        // complain
        error
            << pyre::journal::at(__HERE__)
            << "while allocating device memory for the correlation matrix: "
            << description << " (" << status << ")"
            << pyre::journal::endl;
        // and bail
        throw std::bad_alloc();
    }
    // show me
    channel
        << pyre::journal::at(__HERE__)
        << "allocated " << footprint << " bytes for the correlation matrix at "
        << dCorrelation
        << pyre::journal::endl;

    // get the dimension of the reference tiles
    auto refDim = _refLayout.shape()[0];
    // get the dimension of the target tiles
    auto tgtDim = _tgtLayout.shape()[0];
    // get the dimension of the correlation matrix
    auto corDim = _corLayout.shape()[0];

    // engage
    kernels::correlate(rArena, refStats, tgtStats,
                       _pairs,
                       _refCells, _tgtCells, _corCells, refDim, tgtDim, corDim,
                       dCorrelation);

    // all done
    return dCorrelation;
}


// end of file
